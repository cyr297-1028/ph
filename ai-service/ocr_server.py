import uvicorn
from fastapi import FastAPI, File, UploadFile
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info
import torch
import os
import shutil
import json

app = FastAPI()

# ==========================================
# 1. åˆå§‹åŒ– SMR-R1 æ¨¡å‹ (æ›¿ä»£ PaddleOCR)
# ==========================================
MODEL_PATH = "mrlijun/SMR-R1"  # HuggingFace æ¨¡å‹ IDï¼Œç¬¬ä¸€æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½

print("â³ æ­£åœ¨åŠ è½½ SMR-R1 æ¨¡å‹ (è¿™éœ€è¦è¾ƒå¤šæ˜¾å­˜)...")
try:
    # åŠ è½½æ¨¡å‹ (è‡ªåŠ¨é€‚é…æ˜¾å¡)
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto" 
    )
    # åŠ è½½å¤„ç†å™¨ (è´Ÿè´£å¤„ç†å›¾ç‰‡å’Œæ–‡å­—)
    processor = AutoProcessor.from_pretrained(MODEL_PATH)
    print("âœ… SMR-R1 æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ (è¯·æ£€æŸ¥æ˜¾å­˜æˆ–CUDAé…ç½®): {e}")
    model = None
    processor = None

@app.post("/ocr/medical_report")
async def analyze_medical_report(file: UploadFile = File(...)):
    # 1. ä¿å­˜å›¾ç‰‡åˆ°æœ¬åœ°
    save_dir = "temp_uploads"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    file_path = os.path.join(save_dir, file.filename)
    
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    print(f"âœ… å›¾ç‰‡å·²æ¥æ”¶: {file_path}")

    if model is None:
        return {"code": 500, "msg": "æ¨¡å‹æœªèƒ½æˆåŠŸå¯åŠ¨ï¼Œæ— æ³•å¤„ç†è¯·æ±‚ã€‚"}

    try:
        # ==========================================
        # 2. æ„é€  Prompt (æç¤ºè¯)
        #    åœ¨è¿™é‡Œå‘Šè¯‰æ¨¡å‹ï¼šä½ è¦åˆ†ç±»ï¼Œè¿˜è¦ç»“æ„åŒ–æå–
        # ==========================================
        prompt_text = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œå®Œæˆä»¥ä¸‹ä»»åŠ¡ï¼š
        1. ã€åˆ†ç±»ã€‘ï¼šåˆ¤æ–­è¿™å¼ å›¾ç‰‡çš„ç±»å‹ï¼ˆå¦‚ï¼šè¡€å¸¸è§„æ£€éªŒæŠ¥å‘Šã€ç”ŸåŒ–æ£€éªŒæŠ¥å‘Šã€å°¿æ¶²åˆ†ææŠ¥å‘Šã€å¤„æ–¹å•ã€å…¶ä»–ï¼‰ã€‚
        2. ã€æå–ã€‘ï¼šæå–è¡¨æ ¼ä¸­çš„æ‰€æœ‰æ£€æµ‹é¡¹ç›®ã€‚
        
        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ ¼å¼è¾“å‡ºç»“æœï¼Œä¸è¦åŒ…å« Markdown æ ¼å¼ï¼š
        {
            "report_type": "æŠ¥å‘Šç±»å‹",
            "items": [
                {"name": "é¡¹ç›®åç§°", "result": "ç»“æœæ•°å€¼", "unit": "å•ä½", "ref_range": "å‚è€ƒèŒƒå›´", "arrow": "å¼‚å¸¸ç®­å¤´(â†‘/â†“/æ— )"}
            ],
            "patient": {
                "name": "å§“å",
                "sample_time": "é‡‡æ ·æ—¶é—´"
            }
        }
        """

        # ==========================================
        # 3. è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç† (ç«¯åˆ°ç«¯)
        # ==========================================
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": file_path},
                    {"type": "text", "text": prompt_text},
                ],
            }
        ]

        # é¢„å¤„ç†è¾“å…¥
        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = processor(
            text=[text],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda") # å‘é€åˆ°æ˜¾å¡

        # ç”Ÿæˆç»“æœ
        print("â³ SMR-R1 æ­£åœ¨æ€è€ƒå’Œæå–...")
        generated_ids = model.generate(**inputs, max_new_tokens=2048) # å…è®¸ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        generated_ids_trimmed = [
            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = processor.batch_decode(
            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
        )[0]

        print(f"ğŸ§  æ¨¡å‹åŸå§‹è¾“å‡º: {output_text[:100]}...")

        # ==========================================
        # 4. è§£æç»“æœ (å°†æ¨¡å‹çš„æ–‡æœ¬è½¬å› JSON)
        # ==========================================
        try:
            # æœ‰æ—¶å€™æ¨¡å‹ä¼šè¾“å‡º ```json ... ```ï¼Œéœ€è¦æ¸…æ´—ä¸€ä¸‹
            clean_json_str = output_text.replace("```json", "").replace("```", "").strip()
            result_json = json.loads(clean_json_str)
            
            # é€‚é…ä½ å‰ç«¯éœ€è¦çš„æ ¼å¼
            final_data = {
                "entities": result_json.get("patient", {}),  # å¯¹åº”ä½ åŸæ¥çš„ entities
                "tables": [], # SMR-R1 ç›´æ¥æå–äº†ç»“æ„åŒ– itemsï¼Œå¯èƒ½ä¸éœ€è¦åŸæ¥çš„ html è¡¨æ ¼äº†ï¼Œæˆ–è€…ä½ å¯ä»¥è‡ªå·±æ‹¼ä¸€ä¸ª html
                "structured_items": result_json.get("items", []), # æ–°å¢ï¼šç»“æ„åŒ–çš„é¡¹ç›®åˆ—è¡¨
                "doc_type": result_json.get("report_type", "æœªçŸ¥") # æ–°å¢ï¼šè‡ªåŠ¨åˆ†ç±»ç»“æœ
            }
            
            return {"code": 200, "data": final_data}

        except json.JSONDecodeError:
            print("âš ï¸ æ¨¡å‹è¾“å‡ºçš„ä¸æ˜¯æ ‡å‡† JSONï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            return {"code": 200, "data": {"raw_text": output_text}}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"code": 500, "msg": f"AI æ¨ç†å¤±è´¥: {str(e)}"}

if __name__ == '__main__':
    uvicorn.run(app, host="0.0.0.0", port=8000)